---
title: "EDS 232 Communities"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, palmerpenguins, skimr, tibble)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("penguins")

# look at documentation in RStudio
if (interactive())
  help(penguins)

# show data table
datatable(penguins)
```

```{r}
# skim the table for a summary
skim(penguins)
```


```{r}
# remove the rows with NAs
penguins <- na.omit(penguins)

# plot bill length vs width, species naive
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()
```
```{r}
# plot bill length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos
```
```{r}
# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# show cluster result
penguins_k
```

```{r}
# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)
```

Question: Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.

Answer: The kmeans() technique seems to place more importance on bill length when clustering. This works well in capturing the shorter bill length of the Adelie Penguins but not with the Chinstrap and Gentoo Penguins. The distinction between Chinstrap and Gentoo Penguins seems to be more reliant on bill depth, and therefore is not captured well in the clustering technique. 

```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos
```

```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

```{r}
librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

Question: What are the rows and columns composed of in the dune data frame?

Answer: dune is a data frame of observations of 30 species at 20 sites.


```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```

```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```

```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

Question: In your own words, how does Bray Curtis differ from Euclidean distance?


Answer: Bray-Curtis distance is calculated based on dissimilarity regarding counts of shared species (value range 0-1). Euclidean distance is calculated using the pythagorean theorem, and relies heavily on abundance of species opposed to counts of shared species (value = anything > 0)


```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
```

```{r}
as.matrix(d)[1:5, 1:5]
```

```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)
```


Question: Which function comes first, vegdist() or hclust(), and why? 

Answer: The function vegdist() comes before hclust() in order to generate a distance matrix before clustering.


```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac
```

```{r}
# Dendrogram plot of hc2
plot(hc2, which.plot = 2)
```

Question: In your own words, how does hclust() differ from agnes()?


Answer: These two functions seem similiar. Compared to hclust, agnes has the following features: (a) it yields the agglomerative coefficient which measures the amount of clustering structure found; and (b) apart from the usual tree it also provides the banner. 
```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

Question: Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?

Answer: The ward model has the highest agglomerative coefficient of the four methods (0.69), and is therefore the best.

```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac

```

```{r}
# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc

```
Question: In your own words how does agnes() differ from diana()
	
Answer: Diana creates a divisive hierarchial clustering compared to the agglomerative hierarchial clustering produced by agnes()	

```{r}
librarian::shelf(factoextra)

# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```
Question: How do the optimal number of clusters compare between methods for those with a dashed line?

Answer: The optimal number of clusters returned by the Silhouette method is 4. The optimal number of clusters in the Gap Statistic method is 3
```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

```{r}

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```


```{r}
# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")

```

Question: In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection?

Answer: The height of their shared connection is the biggest determinant of relatedness between observations. 